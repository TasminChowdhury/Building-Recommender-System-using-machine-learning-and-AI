# Building-Recommender-System-using-machine-learning-and-AI

Extrawork from user,
1. put a thumbsup/rate 1-10/ write a comment. not everyone likes to this, so we have sparse data which makes low quality recommendation system.
Also, some people are critical, cultural differences.
2. Implicit behavior: which links users are clicking, those are positive. click data is great, so much of it, no problem with sparsity. sometimes 
not reliable, people click by accident. things people purchase, is true indication of interests. Also things user consume, youtube can see how
many minitues user spent on something

*** hard part is to buil the databse for item similarities

**K-fold validation algo -> prevents overfitting
* we want to recommend people they havent seen and they might find interesting
** Accuracy metric:
mean absolute error (MAE)
Root mean square error(RMSE)
RMSE does not really matter, what matters which movies you put infront of your user and how they react. 
***evaluating top-n recommender
hit rate: if user actually hit like to a video. sum(all hits in top n from every users) / # of users. but measuting hit rate is tricky
We can't use the same train test or cross validation approach we used for measuring accuracy, because we're not measuring the accuracy on
individual ratings. We're measuring the accuracy of top-end lists for individual users. Now you could do the obvious thing and not split
things up at all, and just measure hit rate directly on top end recommendations, created by a recommender system, that was trained on all


of the data you have. But, technically, that's cheating. You generally don't want to evaluate a system using data that it was trained with.
I mean think about it. You could just recommend the actual top ten movies rated by each user, using the training data, and achieve a hit
rate of 100%. So a clever way around this is called leave-one-out cross validation. What we do is compute the top end recommendations for 
each user in our training data, and intentionally remove one of those items from that users training data. We then test our recommenders

system's ability to recommend that item that was left out in the top end results it creates for that user in the testing phase. So we 
measure our ability to recommend an item in a top end list for each user that was left out from the training data. That's why it's 
called leave-one-out. The trouble is, it's a lot harder to get one specific movie right, while testing, than to just get one of the
end recommendations. So hit rate with leave-one-out tends to be very small, and difficult to measure, unless you have a very large
data sets to work with. But it's a much more user-focused metric when you know your recommender system will be producing top end lists 
in the real world, which most of them do. A variation on hit rate is average reciprocal hit rate, or ARHR for short. This metric is just
like hit rate, but it accounts for where in the top end list your hits appear. So you end up getting more credit for successfully
recommending an item in the top slot, than in the bottom slot. Again, this is a more user-focused metric, since users tend to focus on 
the beginning of lists. The only difference is that instead of summing up the number of hits, we sum up the reciprocal rank of each hit.
So if we successfully predict a recommendation in slot three, that only counts as one-third. But a hit in slot one of our top end
recommendations receives the full weight of 1.0. Whether this metric makes sense for you, depends a lot on how your top end 
recommendations are displayed. If the user has to scroll or paginate to see the lower items in your top end list, then it makes sense to
penalize good recommendations that appear too low in the list, where the user has to work to find them. Another twist is cumulative hit
rank. Sounds fancy, but all it means is that we throw away hits if our predicted ratings below some threshold. The idea is that we 
shouldn't get credit for recommending items to a user that we think they won't actually enjoy. So in this example, if we had a cutoff of
three stars, we'd throw away the hits for the second and fourth items in these test results, and our hit rate metric wouldn't count 
them at all. Yet another way to look at hit rate, is to break it down by predicted rating score. It can be a good way to get an idea of
the distribution of how good your algorithm thinks recommended movies are, that actually get a hit. Ideally, you want to recommend movies
that they actually liked. And breaking down the distribution gives you some sense of how well you're doing in more detail. This is called
rating hit rate, or rHR, for short. So those are all different ways to measure the effectiveness of top end recommenders offline. The
world of recommender systems would probably be a little bit different if Netflix awarded the Netflix prize on hit rate, instead of RMSE.
It turns out that small improvements in RMSE can actually result in large improvements to hit rates, which is what really matters. But 
it also turns out that you can build recommender systems with great hit rates, but poor RMSE scores. And we'll see some of those later 
in this course. So RMSE and hit rate aren't always related.

Coverage, diversity, and novelty
- [Instructor] Accuracy isn't the only thing that matters with recommender systems. There are other things we can measure if they're important to us. For example, coverage. That's just the percentage of possible recommendations that your system is able to provide. Think about the movie lens data set of movie ratings we're using in this course. It contains ratings for several thousand movies, but there are plenty of movies in existence that it doesn't have ratings for. If you were using this data to generate recommendations on say, IMDB, then the coverage of this recommender system would be low, because IMDB has millions of movies in its catalog, not thousands. It's worth noting that coverage can be at odds with accuracy. If you enforce a higher quality threshold on the recommendations you make, then you might improve your accuracy at the expense of coverage. Finding the balance of where exactly you're better off recommending nothing at all can be delicate. Coverage can also be important to watch because it gives you a sense of how quickly new items in your catalog will start to appear in recommendations. When a new book comes out on Amazon, it won't appear in recommendations until at least a few people buy it, therefore establishing patterns with the purchase of other items. Until those patterns exist, that new book will reduce Amazon's coverage metric. Another metric is called diversity. You can think of this as a measure of how broad a variety of items your recommender system is putting in front of people. An example of low diversity would be a recommender system that just recommends the next books in a series that you've started reading, but doesn't recommend books from different authors or movies related to what you've read. This may seem like a subjective thing, but it is measurable. Many recommender systems start by computing some sort of similarity metric between items. So we can use these similarity scores to measure diversity. If we look at the similarity scores of every possible pair in a list of top end recommendations, we can average them to get a measure of how similar the recommended items in the list are to each other. We can call that measure S. Diversity is basically the opposite of average similarity, so we subtract it from one to get a number associated with diversity. It's important to realize that diversity, at least in the context of recommender systems, isn't always a good thing. You can achieve very high diversity by just recommending completely random items. But those aren't good recommendations by any stretch of the imagination. Unusually high diversity scores mean that you just have bad recommendations more often than not. You always need to look at diversity alongside metrics that measure the quality of the recommendations as well. Similarly, novelty sounds like a good thing, but often it isn't. Novelty is a measure of how popular the items are that you're recommending. And again, just recommending random stuff would yield very high novelty scores since the vast majority of items are not top sellers. Although novelty is measurable, what to do with it is in many ways subjective. There's a concept of user trust in a recommender system. People want to see at least a few familiar items in their recommendations that make them say yeah, that's a good recommendation for me. This system seems good. If you only recommend things people have never heard of, they may conclude that your system doesn't really know them, and they may engage less with your recommendations as a result. Also, popular items are usually popular for a reason. They're enjoyable by a large segment of the population, so you would expect them to be good recommendations for a large segment of the population who hasn't read or watched them yet. If you're not recommending some popular items, you should probably question whether your recommender system is really working as it should. This is an important point. You need to strike a balance between familiar popular items and what we call serendipitous discovery of new items the user has never heard of before. The familiar items establish trust with the user and the new ones allow the user to discover entirely new things that they might love. Novelty is important though because the whole point of recommender systems is to service items in what we call the long tail. Imagine this is a plot of the sales of items of every item in your catalog sorted by sales. So the number of sales or popularity is on the Y axis, and all the products are along the X axis. You almost always see an exponential distribution like this. Most sales come from a very small number of items, but taken together, the long tail makes up a large amount of sales as well. Items in that long tail, the yellow part in the graph, are items that cater to people with unique niche interests. Recommender systems can help people discover those items in the long tail that are relevant to their own unique niche interests. If you can do that successfully, then the recommendations your system makes can help new authors get discovered, can help people explore their own passions, and make money for whoever you're building the system for as well. Everybody wins. When done right, recommender systems with good novelty scores can actually make the world a better place. But again, you need to strike a balance between novelty and trust. As I said, building recommender systems is a bit of an art, and this is an example of why.

Churn, responsiveness, and A/B tests
- Another thing we can measure is churn. How often do the recommendations for a user change? In part, churn can measure how sensitive your recommender system is to new user behavior. If a user rates a new movie, does that substantially change their recommendations? If so, then your churn score will be high. Maybe just showing someone the same recommendations too many times is a bad idea in itself. If a user keeps seeing the same recommendation but doesn't click on it, at some point should you just stop trying to recommend it and show the user something else instead? Sometimes a little bit of randomization in your top end recommendations can keep them looking fresh and expose your users to more items than they would have seen otherwise but just like diversity and novelty, high churn is not in itself a good thing. You could maximize your churn metric by just recommending items completely at random and of course those would not be good recommendations. All of these metrics need to be looked at together and you need to understand the trade offs between them. One more metric is responsiveness. How quickly does new user behavior influence your recommendations? If you rate a new movie, does it effect your recommendations immediately or does it only effect your recommendations the next day after some nightly job runs? More responsiveness would always seem to be a good thing but in the world of business you have to decide how responsive your recommender really needs to be. Since recommender systems that have instantaneous responsiveness are complex, difficult to maintain, and expensive to build, you need to strike your own balance between responsiveness and simplicity. We've covered a lot of different ways to evaluate your recommender system, MAE, RMSE, hit rate in various forms, coverage, diversity, novelty, churn, and responsiveness, so how do you know what to focus on? Well the answer is that it depends. It may even depend on cultural factors. Some cultures may want more diversity and novelty in their recommendations than others, while other cultures may want to just stick with things that are familiar with them. It also depends on what you're trying to achieve as a business and usually a business is just trying to make money which leads to one more way to evaluate recommender systems that is arguably the most important of all. That's doing online AB tests to tune your recommender system using your real customers and measuring how they react to your recommendations. You can put recommendations from different algorithms in front of different sets of users and measure if they actually buy, watch, or otherwise indicate interest in the recommendations you've presented. By always testing changes to your recommender system using controlled online experiments, you can see if they actually cause people to discover and purchase more new things than they would have otherwise. That's ultimately what matters to your business and it's ultimately what matters to your users too. None of the metrics we've discussed matter more than how real customers react to the recommendations you produce in the real world. You can have the most accurate rating predictions in the world but if customers can't find new items to buy or watch from your system they will be worthless from a practical standpoint. If you test a new algorithm and it's more complex than the one it replaced then you should discard it if it does not result in a measurable improvement in users interacting with the recommendations you present. Online tests can help you to avoid introducing complexity that adds no value and remember complex systems are difficult to maintain. So remember offline metrics such as accuracy, diversity, and novelty can all be indicators you can look at while developing recommender systems offline but you should never declare victory until you've measured a real impact on real users from your work. Systems that look good in an offline setting often fail to have any impact in an online setting that is in the real world. User behavior is the ultimate test of your work. There is a real effect where often accuracy metrics tell you that an algorithm is great only to have it do horribly in an online test. YouTube studied this and calls it the surrogate problem. Accurately predicted ratings don't necessarily make for good video recommendations. YouTube said in one of their papers and I quote, "There is more art than science in selecting the surrogate problem for recommendations." What they mean is you can't always use accuracy as a surrogate for good recommendations. Netflix came to the same conclusion which is why they aren't really using the results of that one million dollar prize for accuracy that they paid out. At the end of the day, the results of online AB tests are the only evaluation that matters for your recommendation system. Another thing you can do is just straight up ask your users if they think specific recommendations are good. Just like you can ask for explicit feedback on items with ratings you can ask users to rate your recommendations too. This is called measuring the perceived quality of recommendations and it seems like a good idea on paper since as you've learned, defining what makes a good recommendation is by no means clear. In practice though, it's a tough thing to do. Users will probably be confused over whether you're asking them to rate the item or rate the recommendation so you won't really know how to interpret this data. It also requires extra work from your customers with no clear payoff for them so you're unlikely to get enough ratings on your recommendations to be useful. It's best to just stick with online AB tests and measure how your customers vote with their wallets on the quality of your recommendations.

Review ways to measure your recommender
Your first question is, "Which metric was used to evaluate the Netflix prize?" By putting a one million dollar bounty on improving a specific metric, Netflix reshaped the world of recommender system research to focus on that metric, for better or worse. What was it? The answer is, "Root mean squared error." Or "RMSE." But as we've said, accuracy isn't everything in the real world. Users don't care how accurately you can predict how they rated movies they've already seen, they care about your ability to show them new things that they will love. Arguably, Netflix should've focused on a metric more focused on top end recommendations, which leads to our next question. "What's a metric for top-n recommenders that accounts for the rank of the predicted items?" In truth, there's more than one, but there's only one that we've talked about in this course so far. The answer is, "Average reciprocal hit rank," or "ARHR" for short. It measures our ability to recommend items that actually appeared in a users top-n highest rated movies, and gives more weight to these hits when they appear near the top of top-n list. We talked about the long tail, and how there is real social value in recommending items that are relatively obscure yet relevant to someones unique niche interests. What metric tells us about how popular or obscure our recommendations are. The answer is, "Novelty." Which is just a measure of the popularity rank of the items we recommend on the whole. And again, a balance must be struck between recommending popular items that establish trust in the recommender system, and more obscure items that may lead to serendipitous discovery from the users. "Which metric would tell us if we're recommending the same types of things all the time?" For example, recommending every Star Wars movie because I've watched one of them. Probably isn't a very serendipitous experience for the user. The answer is, "Diversity." Which is one minus the average similarity score between the items you recommend to each user. And again, you need to be careful with this metric. Never look at diversity alone, because you can get the highest diversity score by just recommending random stuff to people. And that's not a good recommender system. High diversity is often just a sign of bad recommendations. Finally, "Which metric matters more than anything?" It bears repeating. That's the results of online a/b tests. None of the offline metrics we discussed can take the place of measuring how real users react to the recommendations you put in front of them. You just can't simulate what goes on inside the heads of real people when they see new things. You have to try out your changes on them in the real world, and in a controlled manner to see what works, and what doesn't.
